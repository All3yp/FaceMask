# Getting Started - CoreML

There are 2 key elements in the main export directory. Your model in CoreML format (.mlmodel) and signature.json which contains information about your Lobe project. With these, you are ready to use your model!

##

Drag the model into your Xcode project. A class will be autogenerated using your model name and contain helpers for integrating the model in your application.

We will need a way to access our model, create then send a request.

```swift
import CoreML
import Vision

/* Prepare the model and request */
let model = try? VNCoreMLModel(for: SavedModel().model) else { return }
let request = VNCoreMLRequest(model: model) { (finishReq, err) in
    self.processClassifications(for: finishReq, error: err)

/* Where image is a UIImage, convert prior to prediction */
let img = CIImage(image: image) else { return }

/* Get predicted classification for the given image */
try VNImageRequestHandler(ciImage: img).perform([request])
```

The result of the request will have information about the predicted class and confidence levels. Process the output to get the information you want for your app.

```swift
func processClassifications(for request: VNRequest, error: Error?) {
    DispatchQueue.main.async {
        guard let results = request.results else {
            self.classificationLabel.text = "Unable to classify image.\n\(error!.localizedDescription)"
            return
        }
        let classifications = results as! [VNClassificationObservation]

        if classifications.isEmpty {
            self.classificationLabel = "Nothing recognized."
        } else {
            /* Display top classifications ranked by confidence in the UI. */
            let topClassifications = classifications.prefix(1)
            self.classificationLabel = topClassifications[0].identifier
            self.confidence = topClassifications[0].confidence
            }
        }
```
